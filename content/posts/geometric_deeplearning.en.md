+++
date = '2026-01-17T2:53:59+08:00'
draft = false
math = true
tags = ['geometric deep learning', 'machine learning']
title = 'An introduction to Geometric Deep Learning'
[cover]
    image = "/geometric_deeplearning/geocover.png"
    relative = false
+++

## Evolution of Geometric Deep Learning

Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid's *Elements*, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.

Towards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.

A way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.


## A Brief introduction to Deep Learning
Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.

## The goal of Geometric Deep Learning
While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.

Such a 'geometric unification' endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.

## Learning in High Dimensions
Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\mathcal{X} \times \mathcal{Y}$.

Let us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\theta \in \Theta\\}$.

modern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\widetilde{f} \in F$ satisfies $\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.


The performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$:
$$ \mathcal{R} = \mathbb{E}_{(x,y) \sim P}[L(\widetilde{f}(x), f(x))] $$

Such constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\mathcal{F}$ with universal approximation capabilities, we can define a complexity function:
$$c: \mathcal{F} \rightarrow \mathbb{R}$$
defining our interpolation problem as:
$$\widetilde{f} = \arg\min_{g \in \mathcal{F}} c(g) \quad s.t. \quad \widetilde{f}(x_i) = f(x_i), \quad i = 1, . . . , N$$

![Universal Approximation Theorem Diagram](/geometric_deeplearning/geometric01.png)

Therefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function.
The benefit of this definition is that $\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.

## Geometric Priors
To overcome the "curse of dimensionality" of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.

For example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.


## Symmetry and Invariance
In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc.
For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.

### Symmetry Groups
Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.

Symmetry operations form a group, called a symmetry group, proven as follows:

1. **Closure**: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data.
2. **Associativity**: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \circ g_2) \circ g_3 = g_1 \circ (g_2 \circ g_3)$, because the order of transformations does not affect the final result.
3. **Identity**: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \circ g = g \circ e = g$. This identity operation corresponds to performing no transformation.
4. **Inverse**: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \circ g^{-1} = g^{-1} \circ g = e$. This inverse operation corresponds to undoing the transformation.

Note: $g \circ h$ denotes applying transformation $g$ first, then transformation $h$.

Let's take an equilateral triangle as an example to illustrate the concept of symmetry groups:
![Symmetry group of an equilateral triangle](/geometric_deeplearning/geometric02.png)

In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle.
Mathematically, we can view these operations as mappings:
$$g: \mathcal{X} \rightarrow \mathcal{X}$$
where $\mathcal{X}$ is the data space. For each element $g \in G$ in a symmetry group $G$, we have a corresponding mapping $g$.
For example, $D_3$ can also be represented as a matrix group:
$$
D_3 = \left\\{ 
  \begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \end{pmatrix}, 
  \begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 3 & 1 \end{pmatrix}, 
  \begin{pmatrix} 1 & 2 & 3 \\\\ 3 & 1 & 2 \end{pmatrix}, 
  \begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 3 & 2 \end{pmatrix}, 
  \begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 1 & 3 \end{pmatrix}, 
  \begin{pmatrix} 1 & 2 & 3 \\\\ 3 & 2 & 1 \end{pmatrix} 
\right\\}
$$

## References
1. Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., & Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. *IEEE Signal Processing Magazine*, 34(4), 18-42.
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT press.
3. Cohen, T. S., & Welling, M. (2016). Group equivariant convolutional networks. In *International conference on machine learning* (pp. 2990-2999). PMLR.
4. Kondor, R., & Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In *International conference on machine learning* (pp. 2747-2755). PMLR.
5. Wood, T., & Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. *Neural computation*, 8(5), 1003-1013.
6. Mallat, S. (2016). Understanding deep convolutional networks. *Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences*, 374(2065), 20150203.
7. Lee, J. M. (2013). *Introduction to smooth manifolds*. Springer Science & Business Media.
