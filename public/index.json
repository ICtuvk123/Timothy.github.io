[{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"","title":""},{"content":"","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"For nearly two millenia since Euclid’s Elements, the word “geometry” has been synonymous with Euclidean geometry, as no other types of geometry existed. Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann. Towards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003cp\u003eFor nearly two millenia since Euclid’s Elements, the word “geometry” has been synonymous with Euclidean geometry, as no other types of geometry existed.\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\nTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"For nearly two millenia since Euclid’s Elements, the word “geometry” has been synonymous with Euclidean geometry, as no other types of geometry existed.Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.Towards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003cp\u003eFor nearly two millenia since Euclid’s Elements, the word “geometry” has been synonymous with Euclidean geometry, as no other types of geometry existed.Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.Towards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {{f_\\theta \\in \\Theta}}$.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {{f_\\theta \\in \\Theta}}$.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $˜f ∈ F$ satisfies $˜f(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $f^{\\approx} ∈ F$ satisfies $f^{\\approx}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $f^{\\tilde} \\in F$ satisfies $f^{\\tilde}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $f^{\\tilde} \\in F$ satisfies $f^{\\tilde}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $f^{} \\in F$ satisfies $f^{}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $f^{} \\in F$ satisfies $f^{}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $f^{\\sim} \\in F$ satisfies $f^{\\sim}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = {(x_i, y_i)}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = {f_\\theta \\in \\Theta}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f}$ $\\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f}$ $\\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f}$ $\\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all i = 1, . . . , N.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f}$ $\\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry.This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f}$ $\\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The\nancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e,\nEuclid’s monopoly came to an end in the nineteenth century, with examples\nof non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,\nand Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged\ninto disparate fields, with mathematicians and philosophers debating the\nvalidity of and relations between these geometries as well as the nature of\nthe “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"几何深度学习的演变 几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\n到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\n年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\n深度学习简介 值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\n几何深度学习的目标 虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\n这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\n高维学习 监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\n让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\n现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\n学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"几何深度学习的演变\"\u003e几何深度学习的演变\u003c/h2\u003e\n\u003cp\u003e几千年来，几何学一直是人类知识的基本组成部分。古希腊人通过欧几里得的《几何原本》将几何学研究形式化。欧几里得的垄断地位在十九世纪结束了，罗巴切夫斯基、波尔约、高斯和黎曼构建了非欧几里得几何的例子。\u003c/p\u003e\n\u003cp\u003e到了那个世纪末，这些研究已经分化成不同的领域，数学家和哲学家们争论这些几何的有效性和相互关系，以及“唯一真实几何”的本质。\u003c/p\u003e\n\u003cp\u003e年轻的数学家菲利克斯·克莱因指出了摆脱这种困境的出路。克莱因提议将几何学作为不变量的研究，即在某类变换（称为几何的对称性）下保持不变的性质。这种方法通过表明当时已知的各种几何可以通过选择适当的对称变换来定义（使用群论语言形式化），从而创造了清晰度。\u003c/p\u003e\n\u003ch2 id=\"深度学习简介\"\u003e深度学习简介\u003c/h2\u003e\n\u003cp\u003e值得注意的是，深度学习的本质建立在两个简单的算法原则之上：第一，表示或特征学习的概念，即适应性的、通常是分层的特征捕捉每个任务的适当规律性概念；第二，通过局部梯度下降进行学习，通常实现为反向传播。\u003c/p\u003e\n\u003ch2 id=\"几何深度学习的目标\"\u003e几何深度学习的目标\u003c/h2\u003e\n\u003cp\u003e虽然在高维空间学习通用函数是一个棘手的估计问题，但大多数感兴趣的任务并不是通用的，并且带有源于物理世界底层低维性和结构的基本预定义规律。\u003c/p\u003e\n\u003cp\u003e这种“几何统一”的努力具有双重目的：一方面，它提供了一个通用的数学框架来研究最成功的神经网络架构，如 CNN、RNN、GNN 和 Transformer。另一方面，它提供了一个建设性的程序，将先验物理知识融入神经架构，并为构建尚未发明的未来架构提供了原则性的方法。\u003c/p\u003e\n\u003ch2 id=\"高维学习\"\u003e高维学习\u003c/h2\u003e\n\u003cp\u003e监督机器学习，在其最简单的形式化中，考虑一组 N 个观测值 $D = \\{(x_i, y_i)\\}_{i=1}^N$，这些观测值是从定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的底层数据分布 P 中独立同分布 (i.i.d.) 抽取的。\u003c/p\u003e\n\u003cp\u003e让我们进一步假设标签 y 是由未知函数 f 生成的，使得 $y_i = f(x_i)$，并且学习问题简化为使用参数化函数类 $F = \\{f_\\theta \\in \\Theta\\}$ 来估计函数 f。\u003c/p\u003e\n\u003cp\u003e现代深度学习系统通常在所谓的插值机制中运行，其中估计的 $\\widetilde{f}$ $\\in F$ 满足 $\\widetilde{f}(x_i) = f(x_i)$ 对于所有 $i = 1, . . . , N$。\u003c/p\u003e\n\u003cp\u003e学习算法的性能是根据从 $P$ 中抽取的样本的预期性能来衡量的，使用损失函数 $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$：\u003c/p\u003e","title":"几何深度学习简介"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$:\n","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"},{"content":"Evolution of Geometric Deep Learning Geometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s Elements, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\nTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\nA way out of this pickle was shown by a young mathematician Felix Klein. Klein proposed approaching geometry as the study of invariants, i.e. properties unchanged under some class of transformations, called the symmetries of the geometry. This approach created clarity by showing that various geometries known at the time could be defined by an appropriate choice of symmetry transformations, formalized using the language of group theory.\nA Brief introduction to Deep Learning Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent, typically implemented as backpropagation.\nThe goal of Geometric Deep Learning While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world.\nSuch a \u0026lsquo;geometric unification\u0026rsquo; endeavour serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.\nLearning in High Dimensions Supervised machine learning, in its simplest formalisation, considers a set of N observations $D = \\{(x_i, y_i)\\}_{i=1}^N$ drawn i.i.d. from an underlying data distribution P defined over $\\mathcal{X} \\times \\mathcal{Y}$.\nLet us further assume that the labels y are generated by an unknown function f, such that $y_i = f(x_i)$, and the learning problem reduces to estimating the function f using a parametrised function class $F = \\{f_\\theta \\in \\Theta\\}$.\nmodern deep learning systems typically operate in the so-called interpolating regime, where the estimated $\\widetilde{f} \\in F$ satisfies $\\widetilde{f}(x_i) = f(x_i)$ for all $i = 1, . . . , N$.\nThe performance of a learning algorithm is measured in terms of the expected performance on new samples drawn from $P$, using loss function $L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$: $$ \\mathcal{R} = \\mathbb{E}_{(x,y) \\sim P}[L(\\widetilde{f}(x), f(x))] $$\nSuch constructed functions can approximate almost any function (Universal Approximation Theorem), but this does not mean we do not need inductive biases to constrain the learning problem. Given a function class $\\mathcal{F}$ with universal approximation capabilities, we can define a complexity function: $$c: \\mathcal{F} \\rightarrow \\mathbb{R}$$ defining our interpolation problem as: $$\\widetilde{f} = \\arg\\min_{g \\in \\mathcal{F}} c(g) \\quad s.t. \\quad \\widetilde{f}(x_i) = f(x_i), \\quad i = 1, . . . , N$$\nTherefore, we not only hope for good function fitting but also low function complexity. Such a complexity function is what we call the norm of the function. The benefit of this definition is that $\\mathcal{F}$ becomes a Banach space (a complete vector space equipped with a norm), allowing us to use tools from functional analysis to study their properties.\nGeometric Priors To overcome the \u0026ldquo;curse of dimensionality\u0026rdquo; of high-dimensional data, we must utilize the physical characteristics of the data (symmetry and scale separation); although the geometry of the data (Domain) may be complex, the signals defined on it constitute a Hilbert space with good mathematical properties, allowing us to still use tools from linear algebra and functional analysis to handle them.\nFor example, in image processing, data is typically represented as signals (pixel values) defined on a two-dimensional Euclidean space $\\mathbb{R}^2$. This space possesses translation and rotation symmetry, meaning that if we translate or rotate the image, its content and structure remain unchanged. Convolutional Neural Networks (CNNs) exploit this translation symmetry, capturing local features through shared weights, thereby improving learning efficiency and generalization ability.\nSymmetry and Invariance In machine learning, symmetry refers to the property that data or tasks remain unchanged under certain transformations. These transformations can be translation, rotation, scaling, etc. For example, in an image classification task, if we translate or rotate an image, the category of the image usually does not change. This invariance is what we hope the model can capture and utilize.\nSymmetry Groups Symmetry can be formalized through group theory. A group is a set equipped with a binary operation that satisfies properties such as closure, associativity, identity, and inverse.\nSymmetry operations form a group, called a symmetry group, proven as follows:\nClosure: If $g_1$ and $g_2$ are symmetry operations, then their composition $g_1 \\circ g_2$ is also a symmetry operation, because applying two symmetry transformations consecutively still preserves the invariance of the data. Associativity: If $g_1$ and $g_2$ are symmetry operations, then for any $g_3$, $(g_1 \\circ g_2) \\circ g_3 = g_1 \\circ (g_2 \\circ g_3)$, because the order of transformations does not affect the final result. Identity: There exists an identity operation $e$ such that for any symmetry operation $g$, $e \\circ g = g \\circ e = g$. This identity operation corresponds to performing no transformation. Inverse: For every symmetry operation $g$, there exists an inverse operation $g^{-1}$ such that $g \\circ g^{-1} = g^{-1} \\circ g = e$. This inverse operation corresponds to undoing the transformation. Note: $g \\circ h$ denotes applying transformation $g$ first, then transformation $h$.\nLet\u0026rsquo;s take an equilateral triangle as an example to illustrate the concept of symmetry groups: In this example, the symmetry group of the equilateral triangle contains six elements: three rotation operations (0 degrees, 120 degrees, 240 degrees) and three reflection operations (perpendicular lines through each vertex). These operations satisfy the four properties of a group, thus forming a group called $D_3$, the dihedral group of the triangle. Mathematically, we can view these operations as mappings: $$g: \\mathcal{X} \\rightarrow \\mathcal{X}$$ where $\\mathcal{X}$ is the data space. For each element $g \\in G$ in a symmetry group $G$, we have a corresponding mapping $g$. For example, $D_3$ can also be represented as a matrix group: $$ D_3 = \\left\\{ \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{pmatrix}, \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\right\\} $$\nReferences Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., \u0026amp; Vandergheynst, P. (2017). Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18-42. Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Cohen, T. S., \u0026amp; Welling, M. (2016). Group equivariant convolutional networks. In International conference on machine learning (pp. 2990-2999). PMLR. Kondor, R., \u0026amp; Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning (pp. 2747-2755). PMLR. Wood, T., \u0026amp; Shawe-Taylor, J. (1996). Representation theory and invariant neural networks. Neural computation, 8(5), 1003-1013. Mallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Lee, J. M. (2013). Introduction to smooth manifolds. Springer Science \u0026amp; Business Media. ","permalink":"http://localhost:1313/posts/geometric_deeplearning/","summary":"\u003ch2 id=\"evolution-of-geometric-deep-learning\"\u003eEvolution of Geometric Deep Learning\u003c/h2\u003e\n\u003cp\u003eGeometry has been a fundamental part of human knowledge for millennia. The ancient Greeks formalized the study of geometry through Euclid\u0026rsquo;s \u003cem\u003eElements\u003c/em\u003e, Euclid’s monopoly came to an end in the nineteenth century, with examples of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss, and Riemann.\u003c/p\u003e\n\u003cp\u003eTowards the end of that century, these studies had diverged into disparate fields, with mathematicians and philosophers debating the validity of and relations between these geometries as well as the nature of the “one true geometry”.\u003c/p\u003e","title":"An introduction to Geometric Deep Learning"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nAbout the language I will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003ch2 id=\"about-the-language\"\u003eAbout the language\u003c/h2\u003e\n\u003cp\u003eI will be writing my posts in Chinese to reach a broader audience. However, I may include English terms and phrases where necessary for clarity. And of course, English versions of some posts may be provided in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e","title":"First Post"}]