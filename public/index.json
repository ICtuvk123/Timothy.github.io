[{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals. Having a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. Crossover ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. Crossover ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. Crossover ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs \u003cdiv\u003e$x^{(1)}, x^{(2)}, \\dots, x^{(m)}$\u003c/div\u003e. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $$x^{(1)}, x^{(2)}, \\dots, x^{(m)}$$$. The population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs \u003cdiv\u003e$$x^{(1)}, x^{(2)}, \\dots, x^{(m)}$$$\u003c/div\u003e. The population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $$x^{(1)}, x^{(2)}, \\dots, x^{(m)}$$\n. The population at a particular iteration is represented as a generation.\rThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs \u003cdiv\u003e\n$$x^{(1)}, x^{(2)}, \\dots, x^{(m)}$$\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $$ x^{(1)}, x^{(2)}, \\dots, x^{(m)} $$\n. The population at a particular iteration is represented as a generation.\rThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs \u003cdiv\u003e\n$$\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\n$$\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs $$ x^{(1)}, x^{(2)}, \\dots, x^{(m)} $$\nThe population at a particular iteration is represented as a generation.\rThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs \u003cdiv\u003e\n$$\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\n$$\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\r\\begin{equation}\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\\end{equation}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\n\\begin{equation}\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n\\end{equation}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. ","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"},{"content":"Introduction Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\nPolicy and Action The agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\nGoal The goal of the agent is to choose a policy π so as to maximize the sum of expected rewards:\n$$\r\\begin{aligned}\rV^\\pi(s_0) = \\mathbb{E}_{a_0, s_1, a_1, \\dots, a_T, s_T \\sim p(\\cdot \\mid s_0, \\pi)} \\left[ \\sum_{t=0}^T R(s_t, a_t) \\right]\r\\end{aligned}\r$$\rwhere $s_0$ is the initial state, $p(\\cdot|s_0, \\pi)$ is the distribution over trajectories induced by the policy π starting from state $s_0$, and $R(s_t, a_t)$ is the reward received after taking action $a_t$ in state $s_t$. and the expectation is wrt:\n$$\r\\begin{aligned}\rp(a_0, s_1, a_1, \\dots, a_T, s_T \\mid s_0, \\pi) \u0026= \\pi(a_0 \\mid s_0) p_{env}(o_1 \\mid a_0) \\vartheta(s_1 = U(s_0, a_0, o_1)) \\\\\r\u0026= \\prod_{t=1}^T \\pi(a_t \\mid s_t) p_{env}(o_{t+1} \\mid a_t) \\vartheta(s_{t+1} = U(s_t, a_t, o_{t+1}))\r\\end{aligned}\r$$\rwhere $p_{env}(o_{t+1} \\mid a_t)$ is the environment\u0026rsquo;s observation model, and $U(s_t, a_t, o_{t+1})$ is the state update function based on the current state, action taken, and observation received.\nEpisodic vs continual tasks If the agent can potentially interact with the environment forever, we call it a continual task. Alternatively, we say the agent is in an episodic task if its interaction terminates once the system enters a terminal state or absorbing state.\nWe define the return for a state at time t to be the sum of expected rewards obtained going forwards, where each reward is multiplied by a discount factor $\\gamma$:\n$$\r\\begin{aligned}\rG_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\r\u0026= \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\\\\r\u0026= R_{t} + \\gamma G_{t+1}\r\\end{aligned}\r$$\rOverview of the architecture The agent and environment interact at each time step t as follows:\nThe agent has a internal state $z_t$ that summarizes its history of interaction with the environment up to time t. The agent selects an action $a_t$ based on its policy $\\pi(z_t)$, which means that $a_t \\sim \\pi(z_t)$. Then it predicts the next state $z_{t+1|t}$ via the predict function P: $z_{t+1|t} = P(z_t, a_t)$ and optionally predicts the resulting observation $\\hat{o}{t+1|t} = O(z{t+1|t})$. The environment has hidden state $w_t$, which is not directly observable by the agent. The environment transitions to a new state $w_{t+1}$ according to its dynamics: $w_{t+1} \\sim M(w_t, a_t)$. The environment generates an observation $o_{t+1} \\sim O(w_{t+1})$ which is sent back to the agent. ","permalink":"http://localhost:1313/posts/reinforcement_learning01/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eReinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL relies on feedback from the environment in the form of rewards or penalties.\u003c/p\u003e\n\u003ch2 id=\"policy-and-action\"\u003ePolicy and Action\u003c/h2\u003e\n\u003cp\u003eThe agent maintains an internal state $z_t$, which it passes to its policy $\\pi$ to choose an action $a_t = \\pi(z_t).$\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"This post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\nHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\nMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\nPopulation Iteration population iteratively improve a population of m designs\n$$\rx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r$$\rThe population at a particular iteration is represented as a generation.\nThe algorithms are designed so that the individuals in the population converge to one or more local minima over multiple generations.\nThe various methods discussed in this post differ in how they generate a new generation from the current generation.\nPopulation mehtods begin with an initial population, which is often generated randomly.The initial population should be spread over the design space to encourage exploration.\nabstract type PopulationMethod end function population_method(M::PopulationMethod, f, desings, k_max) population = init!(M,f,designs) for k in 1:k_max population = iterate!(M, f, population) end return population end The following are there distributions used to generate the initial population:\nUniform Distribution Normal Distribution Cauchy Distribution Genetic Algorithm The genetic algorithm is inspired by the process of natural selection in biological evolution.\nThe design point associated with each individual is represented as a chromosome. At each generation, the chromosomes of the fitter individuals are passed on to the next generation through selection, crossover, and mutation operations.\nstruct GeneticAlgorithm \u0026lt;: PopulationMethod S # SelectionMethod C # CrossoverMethod U # MutationMethod end init!(M::GeneticAlgorithm, f, designs) = designs function step!(M::GeneticAlgorithm, f, population) S, C, U = M.S, M.C, M.U parents = select(S, f.(population)) children = [crossover(C,population[p[1]],population[p[2]]) for p in parents] return [mutate(U, c) for c in children] end Selection Selection chooses individuals from the current population to serve as parents for the next generation. Common selection methods include rank-based selection, tournament selection, and roulette wheel selection.\nrank-based selection sorts individuals based on their fitness and assigns selection probabilities accordingly. tournament selection randomly selects a subset of individuals and chooses the fittest among them as parents. roulette wheel selection assigns selection probabilities proportional to fitness, allowing fitter individuals a higher chance of being selected. abstract type SelectionMethod end # Pick pairs randomly from top k parents struct TruncationSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TruncationSelection, y) p = sortperm(y) return [p[rand(1:t.k, 2)] for i in y] end # Pick parents by choosing best among random subsets struct TournamentSelection \u0026lt;: SelectionMethod k # top k to keep end function select(t::TournamentSelection, y) getparent() = begin p = randperm(length(y)) p[argmin(y[p[1:t.k]])] end return [[getparent(), getparent()] for i in y] end # Sample parents proportionately to fitness struct RouletteWheelSelection \u0026lt;: SelectionMethod end function select(::RouletteWheelSelection, y) y = maximum(y) .- y cat = Categorical(normalize(y, 1)) return [rand(cat, 2) for i in y] end Crossover Crossover combines the chromosomes of parents to form children. As with selection, there are several crossover schemes\nIn single-point crossover, a random crossover point is selected, and the segments of the parents\u0026rsquo; chromosomes are swapped to create children. In two-point crossover, two crossover points are selected, and the segments between these points are exchanged. In uniform crossover, each gene is independently chosen from one of the parents with equal probability. abstract type CrossoverMethod end struct SinglePointCrossover \u0026lt;: CrossoverMethod end function crossover(::SinglePointCrossover, a, b) i = rand(eachindex(a)) return [a[1:i]; b[i+1:end]] end struct TwoPointCrossover \u0026lt;: CrossoverMethod end function crossover(::TwoPointCrossover, a, b) n = length(a) i, j = rand(1:n, 2) if i \u0026gt; j (i,j) = (j,i) end return [a[1:i]; b[i+1:j]; a[j+1:n]] end struct UniformCrossover \u0026lt;: CrossoverMethod p # crossover probability end function crossover(U::UniformCrossover, a, b) return [rand() \u0026gt; U.p ? u : v for (u,v) in zip(a,b)] end struct InterpolationCrossover \u0026lt;: CrossoverMethod λ # interpolant end crossover(C::InterpolationCrossover, a, b) = (1-C.λ)*a + C.λ*b Mutation Mutation introduces random changes to individuals to maintain genetic diversity within the population. Common mutation method is zero-mean Gaussian distribution.\nabstract type MutationMethod end struct DistributionMutation \u0026lt;: MutationMethod λ # mutation rate D # mutation distribution end function mutate(M::DistributionMutation, child) return [rand() \u0026lt; M.λ ? v + rand(M.D) : v for v in child] end GaussianMutation(σ) = DistributionMutation(1.0, Normal(0,σ)) Each gene in the chromosome typically has a small probability λ of being changed. For a chromosome with m genes, this mutation rate is typically set to λ = 1/m, yielding an average of one mutation per child chromosome.\nDifferential Evolution Differential Evolution (DE) is a population-based optimization algorithm that utilizes vector differences for perturbing the population members.\nFor each individual x:\nSelect three distinct individuals a, b, and c from the population. Generate a trial vector by adding the weighted difference between b and c to a: $$ v = a + F \\cdot (b - c) $$ where F is a scaling factor typically in the range [0, 2]. Evaluate the fitness of the trial vector v. If v has a better fitness than x, replace x with v in the next generation; otherwise, retain x. mutable struct DifferentialEvolution \u0026lt;: PopulationMethod p # crossover probability w # differential weight end init!(M::DifferentialEvolution, f, designs) = designs function step!(M::DifferentialEvolution, f, population) p, w = M.p, M.w n, m = length(population[1]), length(population) for x in population a, b, c = sample(population, 3, replace=false) z = a + w*(b-c) x′ = crossover(UniformCrossover(p), x, z) if f(x′) \u0026lt; f(x) x .= x′ end end return population end Particle Swarm Optimization Particle swarm optimization introduces momentum to accelerate convergence toward minima. Each individual, or particle, in the population keeps track of its current position, velocity, and the best position it has seen so far. Momentum allows an individual to accumulate speed in a favorable direction, independent of local perturbations.\nAt each iteration, each individual is accelerated toward both the best position it has seen and the best position found thus far by any individual. The acceleration is weighted by a random term.\nmutable struct Particle x # position v # velocity x_best # best design thus far end mutable struct ParticleSwarm \u0026lt;: PopulationMethod w # inertia c1 # first momentum coefficient c2 # second momentum coefficient V # initial particle velocity distribution best # best overall design thus far, and its value end function init!(M::ParticleSwarm, f, designs) population = [Particle(x,rand(M.V),copy(x)) for x in designs] best = (x=copy(population[1].x), y=Inf) for P in population y = f(P.x) if y \u0026lt; best.y; best = (x=P.x, y=y); end end M.best = best return population end function step!(M::ParticleSwarm, f, population) w, c1, c2, best = M.w, M.c1, M.c2, M.best n = length(best.x) for P in population r1, r2 = rand(n), rand(n) P.x += P.v P.v = w*P.v + c1*r1.*(P.x_best - P.x) + c2*r2.*(best.x - P.x) y = f(P.x) if y \u0026lt; best.y; best = (x=copy(P.x), y=y); end if y \u0026lt; f(P.x_best); P.x_best .= P.x; end end M.best = best return population end Firefly Algorithm The firefly algorithm was inspired by the manner in which fireflies flash their lights to attract mates of the same species. In the firefly algorithm, each individual in the population is a firefly and can flash to attract other fireflies.\nAt each iteration, all fireflies are moved toward all more attractive fireflies. A firefly\u0026rsquo;s attraction is proportional to its performance.\nstruct Firefly \u0026lt;: PopulationMethod α # walk step size β # source intensity brightness # intensity function end init!(M::Firefly, f, designs) = designs function step!(M::Firefly, f, population) α, β, brightness = M.α, M.β, M.brightness m = length(population[1]) N = MvNormal(I(m)) for a in population, b in population if f(b) \u0026lt; f(a) r = norm(b-a) a .+= β*brightness(r)*(b-a) + α*rand(N) end end return population end Cuckoo Search Cuckoo search is another nature-inspired algorithm named after the cuckoo bird, which engages in a form of brood parasitism. Cuckoos lay their eggs in the nests of other birds.\nIn cuckoo search, each nest represents a design point. New design points can be produced using Lévy flights from nests, which are random walks with step-lengths from a heavy-tailed distribution (typically a Cauchy distribution).\nmutable struct CuckooSearch \u0026lt;: PopulationMethod p_s # search fraction p_a # nest abandonment fraction C # flight distribution end function init!(M::CuckooSearch, f, designs) return [(x=x, y=f(x)) for x in designs] end function step!(M::CuckooSearch, f, population) p_s, p_a, C = M.p_s, M.p_a, M.C m, n = length(population), length(population[1].x) m_search = round(Int, m*p_s) m_abandon = round(Int, m*p_a) for i in 1:m_search j, k = rand(1:m), rand(1:m) x = population[j].x + rand(C,n) y = f(x) if y \u0026lt; population[k].y population[k] = (x=x, y=y) end end p = sortperm(population, by=nest-\u0026gt;nest.y, rev=true) for i in 1:m_abandon j = rand(1:m-m_abandon)+m_abandon x′ = population[p[j]].x + rand(C,n) population[p[i]] = (x=x′, y=f(x′)) end return population end Hybrid Methods Many population methods perform well in global search, being able to avoid local minima and finding the best regions of the design space. Unfortunately, these methods do not perform as well in local search in comparison to descent methods.\nSeveral hybrid methods have been developed to extend population methods with descent-based features to improve their performance in local search.\nIn Lamarckian learning, the population method is extended with a local search method that locally improves each individual. The original individual and its objective function value are replaced by the individual’s optimized counterpart. In Baldwinian learning, the same local search method is applied to each individual, but the results are used only to update the individual’s objective function value. Individuals are not replaced but are merely associated with optimized objective function values. Summary Population methods are powerful optimization techniques that leverage a collection of design points to explore the design space effectively. By utilizing mechanisms inspired by natural processes, such as genetic algorithms, differential evolution, and particle swarm optimization, these methods can navigate complex landscapes and avoid local minima. Hybrid approaches that combine population methods with local search techniques further enhance their performance, making them versatile tools for solving a wide range of optimization problems.\n","permalink":"http://localhost:1313/posts/population_method/","summary":"\u003cp\u003eThis post presents a variety of population methods that involve optimization using a collection of design points, called individuals.\u003c/p\u003e\n\u003cp\u003eHaving a large number of individuals distributed throughtout the design space can help the algorithm to explore the design space more effectively.\u003c/p\u003e\n\u003cp\u003eMany population methods are stochastica in nature, and it is generally easy to parallelize the computation.\u003c/p\u003e\n\u003ch2 id=\"population-iteration\"\u003ePopulation Iteration\u003c/h2\u003e\n\u003cp\u003epopulation iteratively improve a population of m designs\u003c/p\u003e\n\u003cdiv\u003e\r\n$$\r\nx^{(1)}, x^{(2)}, \\dots, x^{(m)}\r\n$$\r\n\u003c/div\u003e\r\n\u003cp\u003eThe population at a particular iteration is represented as a generation.\u003c/p\u003e","title":"Population Method"},{"content":"Welcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\nFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\nHappy blogging!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003cp\u003eWelcome to my first blog post! This is where I will share my thoughts, experiences, and updates. Stay tuned for more content!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFeel free to leave comments and share your feedback. Looking forward to connecting with you all!\u003c/p\u003e\n\u003cp\u003eHappy blogging!\u003c/p\u003e","title":"First Post"}]